{
  "source_file": "lec21_history_neural_networks_typednotes.pdf",
  "total_pages": 13,
  "pages": [
    {
      "page_number": 1,
      "text": "CS 486/686 Lecture 21 A brief history of deep learning\n1\nThis summary is based on A ’Brief’ History of Neural Nets and Deep Learning by\nAndrew Kurenkov.\n1\nA brief history of deep learning\nThere is a deep learning tsunami over the past several years.\n• drastic improvements over reigning approaches towards the hardest problems\nin AI\n• massive investments from industry giants such as Google\n• exponential growth in research publications (and ML graduate students)\nThe birth of machine learning\nIn 1957, a psychologist Frank Rosenblatt developed perceptron, a mathematical\nmodel of neurons in our brain.\nA perceptron:\n• Takes binary inputs, which are either data or the output of another percep-\ntron (a nearby neuron).\n• A special bias input has the value of 1.\nIt allows us to compute more\nfunctions using a perceptron.\n• Links between neurons are called synapses and the synapses have different\nstrengths.\nModel this by multiplying each input by a continuous valued weight.\n• The output depends only on inputs.\n• A neuron either fires or not."
    },
    {
      "page_number": 2,
      "text": "CS 486/686 Lecture 21 A brief history of deep learning\n2\nThe output is 1 if the weighted sum is big enough and the output is 0\notherwise.\nActivation function: a non-linear function of the weighted sum.\nIf the\nweighted sum is above the threshold, the output is 1.\ne.g. step function or the sigmoid function.\nThe perceptron\n• based on earlier work by Mcculoch-Pitts\n• can represent AND, OR, and NOT (1943).\n• big deal: believed that AI is solved if computers could perform formal logical\nreasoning.\n• open question: how do we learn a perceptron model? Rosenblatt answered\nthis question.\nLearning a perceptron:\n• An idea by Donald Hebb: the brain learns by forming synapses and changing\nthe strenghths of the synapses between the neurons.\n• Neuron A repeatedly and persistently takes part in firing neuron B, the brain\ngrows to strength the synapse between A and B.\n• For each example, increase the weights if the output is too low and decrease\nthe weights when the output is too high.\nA simple algorithm to learn a perceptron:\n1. Start with random weights in a perceptron.\n2. For a training example, compute the output of the perceptron."
    },
    {
      "page_number": 3,
      "text": "CS 486/686 Lecture 21 A brief history of deep learning\n3\n3. If the output does not match the correct input:\n4. If the correct output was 0 but the actual output was 1, decrease the weights\nthat had an input of 1.\n5. If the correct output was 1 but the actual output was 0, increase the weights\nthat had an input of 1.\n6. Repeat steps 2-5 for all the training examples until the perceptron makes no\nmore mistakes\nRosenblatt implemented perceptrons and showed that they can learn to classify\nsimple shapes correctly with 20x20 pixel-like inputs. – Machine learning was born.\nThe hype around perceptrons\nHow can we use a perceptron for classification tasks with multiple categories? For\nexample, classify handwritten digits.\n• Arrange multiple perceptrons in a layer.\n• The perceptrons receive the same inputs.\n• Each perceptron learns one output of the function. (Does the inputs belong\nto a particular class?)\nArtificial Neural Networks are simply layers of perceptrons/neurons/units. So far,\nour network only has one layer - the output layer.\nThis can be used to classify handwritten digits. Each of the 10 output values\nrepresents a digit. The highest weighted sum produces an output of 1 and others\nproduce an output of 0.\nThe hype around perceptrons:\n• Perceptrons are so simple. — basically linear regression. So cannot solve\nvision or speech recognition problems yet."
    },
    {
      "page_number": 4,
      "text": "CS 486/686 Lecture 21 A brief history of deep learning\n4\n• However, a network of such simple units can be powerful and solve complex\nproblems.\nIn 1958, Rosenblatt said that: Perceptrons might be fired to the planets as me-\nchanical space explorers.\nAI winter\nThe hype irritated other researchers who were skeptical about perceptrons.\n• Big problem in AI: formal logical reasoning. teaching computers to manip-\nulate logical symbols using rules.\n• In 1969, Marvin Minsky (founder of MIT AI lab) and Seymour Papert (di-\nrector of the lab) published a book named Perceptrons - a rigorous analysis\nof the limitations of perceptrons.\nPerceptrons. An Introduction to Computational Geometry. MARVIN MIN-\nSKY and SEYMOUR PAPERT. M.I.T. Press, Cambridge, Mass., 1969.\n• A notable result: impossible to learn an XOR function using a single per-\nceptron. (XOR function is not linearly separable.).\n• They basically said: this approach was a dead end. This publication was\nbelieved to lead to the first AI winter - a freeze to funding and publications.\nWhat did Minsky’s book show?\nWe need a multi-layer network to do useful things\n• To learn an XOR function or other complex functions.\n• Show what a multi-layer neural network looks like.\n• Hidden layers are good for feature extraction.\nFacial recognition: The first hidden layer converts raw pixels to the locations\nof lines, circles, and ovals in the images. The next hidden layer will take the\nlocations of the shapes and determine whether there are faces in the images."
    },
    {
      "page_number": 5,
      "text": "CS 486/686 Lecture 21 A brief history of deep learning\n5\nProblem:\n• Rosenblatt’s learning algorithm does not work for a multi-layer neural net-\nwork.\n• How do we adjust the weights in the middle layers? Use backpropagation.\nBackpropagation for neural nets\n• Calculate the errors in the output layer.\n• Propagate these errors backwards to the previous hidden layer. Blame the\nprevious layer for some of the errors made.\n• Propagate these errors backwards again to the previous hidden layer.\n• When we change weights in any layer, the errors in the output layer changes.\n• Use an optimization algorithm to find weights that minimize the errors.\nHistory of backpropagation:\n• Derived by multiple researchers in the 60s\n• Implemented by Seppo Linnainmaa in 1970.\n• Paul Werbos first proposed that backpropagation can be used for neural\nnets. (analyzed it in depth in his 1974 PhD thesis).\nThe atmosphere at the time during the AI winter:\n• In his 1969 book, Minsky showed that: we need to use multi-layer percep-\ntrons even to represent simple nonlinear functions such as the XOR mapping.\n• Many researchers had tried and failed to find good ways to train multi-layer\nperceptrons."
    },
    {
      "page_number": 6,
      "text": "CS 486/686 Lecture 21 A brief history of deep learning\n6\n• Neural networks were an dead end. Lack of academic interest in this topic.\nThe community lost faith.\n• Paul Werbos did not publish his idea until 1982.\nThe rise of back propagation\nMore than a decade after Werbos’ thesis, the idea of back propagation was finally\npopularized.\nRumelhart, D. E., Hinton, G. E., and Williams, R. J. (1986). Learning represen-\ntations by back-propagating errors. Nature, 323, 533–536.\n• Idea was rediscovered multiple times before this paper\n• The paper stands out for concisely and clearly stating the idea.\n• Finally succeeded in making this idea well-known.\n• Paper is identical to how the concept is explained in textbooks and AI\nclasses.\n• Wrote another in-depth paper to specifically address the problem pointed\nout by Minsky.\nNeural networks are back. We know how to train multi-layer neural networks to\nsolve complex problems.\nNeural networks are back\nWe know how to train multi-layer neural networks. Become super popular. The\nambitions of Rosenblatt seems to be within reach.\n1989 a key finding: A mathematical proof that multi-layer neural networks can\nimplement any function.\nKurt Hornik, Maxwell Stinchcombe, Halbert White, Multilayer feedforward net-\nworks are universal approximators, Neural Networks, Volume 2, Issue 5, 1989,"
    },
    {
      "page_number": 7,
      "text": "CS 486/686 Lecture 21 A brief history of deep learning\n7\nPages 359-366, ISSN 0893-6080, http://dx.doi.org/10.1016/0893-6080(89)90020-\n8.\nIn 1989 a significant real-world application of backpropagation: handwritten zip\ncode recognition.\n• US postal service was desperate to be able to sort mails automatically. Rec-\nognizing messy handwriting was a major challenge.\n• Yann LeCun and others at the AT&T Bell Labs\n• LeCun, Y; Boser, B; Denker, J; Henderson, D; Howard, R; Hubbard, W;\nJackel, L, “Backpropagation Applied to Handwritten Zip Code Recognition,”\nin Neural Computation , vol.1, no.4, pp.541-551, Dec. 1989 89\n• The method was later used for a nationally deployed cheque reading system\nin the mid 90. (Show video.) (At some point in the late 1990s, one of these\nsystems was reading 10 to 20% of all the checks in the US.)\n• Highlighted a key modification of neural networks towards modern machine\nlearning: extracting local features and combining them to form higher order\nfeatures\nConvolutional neural networks:\n• Each neuron extracts a local feature anywhere inside an image.\nForce a\nhidden unit to only combine local sources of information.\nUsed to be that each neuron is passed the entire image. Now each neuron\nis only passed a portion of the image and it’s looking for a local feature in\nthat portion.\n• Passing a filter through an image and only picks up whether the image has\nthis feature (a horizontal line, a 45 degree line, etc). A magnifying glass\nsliding across the image. Capable of only recognizing one thing. Records\nthe location of each match.\n• The following layers combine local features to form higher order features."
    },
    {
      "page_number": 8,
      "text": "CS 486/686 Lecture 21 A brief history of deep learning\n8\nWhy is this a good idea?\n• Without specialized neurons, need to learn the same feature many times in\ndifferent parts of the image.\n• Only one neuron with fewer weights can learn the feature much faster.\nA new winter dawns\nResearchers started using neural networks for many applications\n• Image compression\n• Learning a probability distribution\n• Playing games such as Backgammon, Chess, and Go.\n• Speech recognition\nNeural networks with many layers trained with backpropagation did not work well.\n• Not as well as simpler networks.\n• Backpropagation relies on finding the error at the output layer and succes-\nsively splitting up blame for it for prior layers.\n• With many layers this splitting of blame ends up with either huge or tiny\nnumbers and the resulting neural net just does not work very well.\nIn mid 90s, a new AI winter began.\n• General perception that neural networks do not work well.\n• Computers were not fast enough. The algorithms were not smart enough.\nPeople were not happy."
    },
    {
      "page_number": 9,
      "text": "CS 486/686 Lecture 21 A brief history of deep learning\n9\n• In contrast, support vector machine (similar to a 2-layer neural network)\nwas shown to work better than neural networks. (LeCun showed this for\nhandwritten digit recognition.)\n• A random forest (multiple decision trees) also worked very well.\nA dark time for neural nets (early 2000s):\n• LeCun and Hinton’s papers were routinely rejected from conferences due to\ntheir subjects being neural nets.\n• Hinton found an ally: the Canadian government.\n• The Canadian Institute for Advanced Research (CIFAR) encouraged basic\nresearch without application.\n• Hinton secured funding from CIFAR and moved to Canada in 1987.\n• With modest funding, they continued working.\nA conspiracy was hatched\n• Rebranded the neural nets with the term “deep learning”.\n• A significant breakthrough to rekindle interest in neural networks and started\nthe deep learning movement.\nHinton, G. E., Osindero, S., & Teh, Y. W. (2006). A fast learning algorithm\nfor deep belief nets. Neural computation, 18(7), 1527-1554.\n• Key idea: Neural nets with many layers could be trained well if the initial\nweights are chosen in a clever way rather than randomly.\n(Set the initial weights by training each layer separately using unsupervised\nlearning.)\n• Proof: Hinton and his students applied deep learning to speech recogni-\ntion. On a standard speech recognition dataset, the best performance was"
    },
    {
      "page_number": 10,
      "text": "CS 486/686 Lecture 21 A brief history of deep learning\n10\nachieve a decade ago. They improved on this performance record, which was\nimpressive.\nMohamed, A. R., Sainath, T. N., Dahl, G., Ramabhadran, B., Hinton, G.\nE., & Picheny, M. (2011, May). Deep belief networks using discriminative\nfeatures for phone recognition. In Acoustics, Speech and Signal Processing\n(ICASSP), 2011 IEEE International Conference on (pp. 5060-5063). IEEE.\nThe importance of brute force\nThe benefit of fast parallel computation and lots of training data\n• CPUs hit a celing (can be run in weak parallelism). High-end graphics cards\nwere powerful and can be used in parallel.\n• Collect lots of training data – can prevent overfitting. Neural networks are\nextremely complex. Used on a small data set, it is easy to overfit.\nStudents of Hinton Dahl and Mohamed went to Microsoft and worked on deep\nlearning there.\nAnother student of Hinton Jaitly went to work at Google. Google soon used deep\nlearning to power Android’s speech recognition.\nAndrew Ng and Jeff Dean formed Google brain. They built even bigger neural\nnets (1 billion weights) to train deep belief nets to recognize the most common\nobjects in YouTube videos. The net discovered cats (also good at detecting human\nfaces and human body parts).\nWhy does backpropagation not work well?\nWhy the old approaches did not work well?\nGlorot, X., & Bengio, Y. (2010). Understanding the difficulty of training deep\nfeedforward neural networks. In International conference on artificial intelligence\nand statistics (pp. 249-256)."
    },
    {
      "page_number": 11,
      "text": "CS 486/686 Lecture 21 A brief history of deep learning\n11\n• The default activation function was not a good choice.\nThe non-differentiable function and very simple function f(x)=max(0,x) (rec-\ntified linear unit) is the best.\n• Weights should be chosen in different scales depending on the layers they\nare in.\nIn 2012, Hinton entered the ImageNet competition using deep convolutional neural\nnetworks. They did far better than the next closest entry. Their error rate was\n15.3%, whereas the second closest was 26.2%. CNN gained respect from the vision\ncommunity.\nThe deep learning tsunami began here in 2012. it has been growing and intensi-\nfying to this day. No winter is in sight."
    },
    {
      "page_number": 12,
      "text": "CS 486/686 Lecture 21 A brief history of deep learning\n12\n2\nSummary\n1. (Frank Rosenblatt 1957) A perceptron can be used to represent and learn\nlogic operators (AND, OR, and NOT). At the time, it was widely believed\nthat AI is solved if computers can perform formal logical reasoning.\n2. (Marvin Minsky 1969) published a rigorous analysis of the limitations of\nperceptrons. Minsky made two claims: We need to use multi-layer percep-\ntrons to represent simple non-linear functions such as XOR. No one knows\na good way to train multi-layer perceptrons. This led to the first AI winter\n(70s to 80s).\n3. In 1986, Rumelhart, Hinton and Williams in their Nature article precisely\nand concisely explained how we can use the back-propagation algorithm to\ntrain multi-layer perceptrons.\n4. In mid 90s, people found that multi-layer neural networks trained with back-\npropagation don’t work well compared to simpler models (such as support\nvector machines and random forests). This led to the second AI Winter\naround mid 90s.\n5. In 2006, Hinton, Osindero and Teh showed that backpropagation works if\nthe initial weights are chosen in a smart way. This brought neural networks\nback into the limelight. In 2012, Hinton entered the ImageNet competition\nusing deep convolutional neural networks and did far better than the next\nclosest entry (their error rate was 15.3% rather than 26.2%).\nThe deep\nlearning tsunami continues today.\nLessons learned summarized by Geoffrey Hinton:\n• Our labeled datasets were thousands of times too small.\nComplex neural networks are prone to overfitting. We need lots of training\ndata to prevent overfitting.\n• Our computers were millions of times too slow.\nWe need a lot of computational power to train massive neural networks."
    },
    {
      "page_number": 13,
      "text": "CS 486/686 Lecture 21 A brief history of deep learning\n13\n• We initialized the weights in a stupid way.\nWeights should be chosen in a clever way rather than randomly.\n• We used the wrong type of non-linearity.\nWe should have used a different activation function."
    }
  ]
}