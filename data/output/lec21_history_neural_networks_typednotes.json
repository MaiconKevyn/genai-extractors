{
  "source_file": "lec21_history_neural_networks_typednotes.pdf",
  "content": "CS 486/686 Lecture 21 A brief history of deep learning This summary is based on 'Brief' History of Neural Nets and Deep Learning by Andrew Kurenkov . 1 A brief history of deep learning There is a deep learning tsunami over the past several years. drastic improvements over reigning approaches towards the hardest problems in AI massive investments from industry giants such as Google exponential growth in research publications and ML graduate students) The birth of machine learning In 1957 psychologist Frank Rosenblatt developed perceptron; mathematical model of neurons in our brain perceptron: Takes binary inputs; which are either data or the output of another percep- tron a nearby neuron) the value of 1. It allows more functions perceptron Links between neurons are called synapses and the synapses have different strengths Model this by multiplying each input by a continuous valued weight. The output depends only on inputs. A neuron either fires Or not using\n\nCS 486/686 Lecture 21 A brief history of deep learning The output   is 1 if the  weighted sum is big enough 0 otherwise Activation function: of the weighted sum If the weighted sum is above the threshold, the output is 1. e.g: step function or the sigmoid function. The perceptron based on earlier work by Mcculoch-Pitts can represent AND and NOT (1943) . big deal: believed that AI is solved if computers could perform formal logical reasoning . open question: how do we learn perceptron model?  Rosenblatt answered this question Learning a perceptron: An idea by Donald Hebb: the brain learns by forming synapses and changing the strenghths of the synapses between the neurons _ Neuron A repeatedly and persistently takes in firing neuron B, the brain grows to strength the synapse between A and B. For each example; increase the weights if the output is too low and decrease the weights when the output is too high. simple algorithm to learn perceptron: 1. Start with random weights in a perceptron_ 2 For a training example; compute the output of the perceptron OR, part\n\nCS 486/686 Lecture 21 A brief history of deep learning 3 If the output does not match the correct input: 4. If the correct output was 0 but the actual output was 1, decrease the hts that had an input of 1. 5 . If the correct output was 1 but the actual output was 0, increase the weights that had an input of 1. 6 Repeat steps 2-5 for all the training examples until the perceptron makes no more mistakes Rosenblatt implemented perceptrons and showed that can learn to classify simple shapes correctly with 20x20 pixel-like inputs. Machine learning was born The hype around perceptrons How can we use a perceptron for classification tasks with multiple categories? For example; classify handwritten digits. Arrange multiple perceptrons in The perceptrons receive the same inputs. Each perceptron learns one output of the function: (Does the inputs belong to particular class?) Artificial Neural Networks are simply layers of perceptrons  neurons /units. So our network only has one the output This can be used to classify  handwritten digits. Each of the 10 output values represents digit. The highest  weighted sum produces an output of 1 and others produce an output of 0. The hype around perceptrons: Perceptrons are so simple. basically linear regression. So cannot solve vision or speech recognition problems weigl they layer . far . layer layer yet.\n\nCS 486/686 Lecture 21 A brief history of deep learning However; a network of such simple units can be powerful and solve complex problems In 1958, chanical space explorers. AI winter The hype irritated other researchers who were skeptical about perceptrons problem in AI: formal logical reasoning   teaching computers to manip- ulate logical symbols rules_ In 1969, Marvin Minsky (founder of MIT AI lab) and Seymour Papert (di- rector of the lab) published a book named Perceptrons a rigorous analysis of the limitations of perceptrons. Perceptrons. An Introduction to Computational Geometry.  MARVIN MIN - SKY and SEYMOUR PAPERT. MIT. Press; Cambridge; Mass. = 1969. notable result:   impossible to learn an XOR function single per - ceptron_ (XOR function is not linearly separable) They basically said: this approach was dead end. This publication was believed to lead to the first AI winter a freeze to funding and publications. What did Minsky's book show? We need a multi-layer network to do useful things To learn an XOR function Or other complex functions Show what a multi-layer neural network looks like_ Hidden layers are for feature extraction. Facial recognition: The first hidden layer converts raw pixels to the locations of lines; circles; and ovals in the images. The next hidden will take the locations of the shapes and determine whether there are faces in the images. Big using using good layer\n\nCS 486/686 Lecture 21 A brief history of deep learning Problem: Rosenblatt 's learning algoritlm does not work for a multi-layer neural net- work How do we adjust the weights in the middle layers? Use backpropagation. Backpropagation for neural nets Calculate the errors in the output Propagate these errors backwards to the previous hidden Blame the previous layer for some of the errors made Propagate these errors backwards to the previous hidden When we change weights in any the errors in the output layer changes. Use an optimization algorithm to find weights that minimize the errors History of backpropagation: Derived by multiple researchers in the 60s Implemented by can be used for neural nets (analyzed it in depth in his 1974 PhD thesis) . The atmosphere at the time during the AI winter: In his 1969 book, Minsky showed that: we need to use multi-layer percep- trons even to represent simple nonlinear functions such as the XOR mapping. Many researchers had tried and failed to find ways to train multi-layer perceptrons_ layer layer . again layer . layer , Seppo good\n\nCS 486/686 Lecture 21 A brief history of deep learning Neural networks were an dead end. Lack of academic interest in this topic. The community lost faith. Paul Werbos did not publish his idea until 1982. The rise of back propagation More than a decade after Werbos' thesis; the idea of back propagation was finally popularized. Rumelhart D. E. Hinton; G. E., and Williams; R. J. (1986). Learning represen - tations by back-propagating errors Idea was rediscovered multiple times before this paper The paper stands out for concisely and clearly stating the idea. Finally succeeded in making this idea well-known: Paper is identical to how the  concept is explained in textbooks   and AI classes Wrote another in-depth paper to specifically   address the problem pointed out by Minsky. Neural networks are back. We know how to train multi-layer neural networks to solve complex problems. Neural networks are back We know how to train multi-layer neural networks. Become super popular. The ambitions of Rosenblatt seems to be within reach. 1989 a finding: A mathematical that multi-layer neural networks can implement any function Kurt Hornik. Maxwell Stinchcombe; Halbert White; Multilayer feedforward net- works are universal approximators; Volume 2 Issue 5,1989, proof key\n\nCS 486/686 Lecture 21 A brief history of deep learning 359-366, ISSN 0893-6080, dx.doi.org/10.1016/0893-6080(89)90020- 8 In 1989 significant real-world application of backpropagation:   handwritten zip code recognition US postal service was desperate to be able to sort mails automatically. Rec- ognizing messy handwriting was major challenge: Yann LeCun and others at the AT&T Bell Labs LeCun; Y; Boser; B: Denker_ J; Henderson; D; Howard, R; Hubbard, W Jackel, L; Backpropagation Applied to Handwritten Zip Code Recognition; in Neural Computation vol.1, n0.4, pp.541-551, Dec. 1989 89 The method was later used for nationally deployed cheque reading system in the mid 90. (Show video ) (At some in the late 1990s, one of these systems was reading 10 to 209 of all the checks in the US:) Highlighted a modification of neural networks towards modern machine learning: extracting local features and combining them to form higher order features Convolutional neural networks: Each neuron extracts local feature anywhere inside an image. Force hidden unit to only combine local sources of information. Used to be that each neuron is passed the entire image. Now each neuron is only passed portion of the image and it's looking for local feature in that portion. Passing filter through an image and only picks up whether the image has this feature (a horizontal line; a 45 degree etc) . A magnifying glass sliding across the image. Capable of only recognizing one Records the location of each match The following combine local features to higher order features. Pages http:/ point key line; thing: form layers\n\nCS 486/686 Lecture 21 A brief history of deep learning Why is this a good idea? Without specialized neurons; need to learn the same feature many times in different parts of the image. Only one neuron with fewer weights can learn the feature much faster. new winter dawns Researchers started using neural networks for many applications compression Learning probability distribution Playing games such as Backgammon; Chess; and Go Speech recognition Neural networks with many layers trained with backpropagation did not work well. Not as well as simpler networks. Backpropagation relies on finding the error at the output and succes- sively splitting up blame for it for prior layers. With many layers this splitting of blame ends up with either huge or numbers and the resulting neural net just does not work very well. In mid AI winter began. General perception that neural networks do not work well Computers  were not fast enough. The algorithms were not smart enough. People were not Image layer tiny 90s, happy.\n\nCS 486/686 Lecture 21 A brief history of deep learning In contrast, support vector machine (similar to 2-layer neural network) was shown to work better than neural networks. (LeCun showed this for handwritten digit recognition: random forest multiple decision trees also worked very well dark time neural nets (early 2000s): LeCun and Hinton's papers were routinely rejected from conferences due to their subjects being neural nets_ Hinton found an ally: the Canadian government _ The Canadian Institute for Advanced Research (CIFAR) encouraged basic research without application. Hinton secured funding from CIFAR and moved to Canada in 1987. With modest funding, continued working: A conspiracy was hatched Rebranded the neural nets with the term learning' A significant breakthrough to rekindle interest in neural networks and started the deep learning movement. Hinton; G. E., Osindero; S., & Teh, Y. W. (2006). A fast learning algorithm for deep belief nets_ Neural computation; 18(7), 1527-1554. idea: Neural nets with many layers could be trained well if the initial weights are chosen in clever way rather than randomly. (Set the initial weights by training each separately using unsupervised learning: Proof: Hinton and tion On a standard speech recognition dataset, the best performance was for they deep Key layer\n\nCS 486/686 Lecture 21 A brief history of deep learning 10 achieve a decade ago. They improved on this performance record, which was impressive. Mohamed , A R Sainath. T. N. Dahl, G. Ramabhadran; B., Hinton, G. E. & Picheny; M. (2011, May) . Deep belief networks using discriminative features for phone recognition. In Acoustics; Speech and Signal Processing (ICASSP) , 2011 IEEE International Conference on (pp. 5060-5063) . IEEE . The importance of brute force The benefit of fast parallel computation and lots of training data CPUs hit a (can be run in weak parallelism) . High-end graphics cards were powerful and can be used in parallel. Collect lots of training data can prevent overfitting: Neural networks are extremely complex. Used on small data set, it is easy to overfit Students   of Hinton Dahl and Mohamed went to Microsoft and worked on deep learning there Another student of Hinton Jaitly went to work at Google. Google soon used learning to power Android's speech recognition Andrew Ng and Jeff Dean formed Google brain. nets (1 billion weights) to train deep belief nets to recognize the most common objects in YouTube videos. The net discovered cats also at detecting human faces and human body parts) Why does backpropagation not work well? Why the old approaches did not work well? & Bengio; Y. (2010). Understanding the difficulty of training deep feedforward neural networks In International conference on artificial intelligence and statistics (pp. 249-256)  celing deep They good\n\nCS 486/686 Lecture 21 A brief history of deep learning 1l The default activation function was not a choice The non-differentiable function and very simple function f(x)=max(O,x) (rec- tified linear unit) is the best. Weights should be chosen in different  scales depending on the layers are in- In 2012, Hinton entered the ImageNet competition using deep convolutional neural networks Their error rate was 15.39, whereas the second closest was 26.2%. CNN gained respect from the vision community. The learning tsunami began here in 2012. it has been growing and intensi - fying to this No winter is in sight. good they deep day .\n\nCS 486/686 Lecture 21 A brief history of deep learning 12 2 Summary 1. (Frank Rosenblatt 1957) A perceptron can be to represent and learn logic operators (AND; OR; and NOT). At the time; it was widely believed that AI is solved if computers can perform formal logical reasoning. 2. (Marvin Minsky 1969) published rigorous analysis of the limitations of perceptrons - Minsky made two claims: We need to use multi-layer percep- trons to represent simple non-linear functions such as XOR. No one knows way to train multi-layer perceptrons_ This led to the first AI winter (7Os to 80s) . 3 Williams in their Nature article precisely and concisely explained how we can use the back-propagation algorithm to train multi-layer perceptrons. In mid people found that multi-layer neural networks trained with back - propagation don't work well compared to simpler models (such as support vector machines and random forests) . This led to the second AI Winter around mid 9Os. 5  In 2006, Hinton; Osindero and Teh showed that backpropagation works if the initial weights are chosen in a smart way. This brought neural networks back into the limelight .  In 2012, Hinton entered the ImageNet competition using convolutional neural networks and did far better than the next closest   entry (their error rate was The deep learning tsunami continues today. Lessons learned summarized by Geoffrey Hinton: Our labeled datasets were thousands of times too small. Complex neural networks are prone to overfitting: We need lots of training data to prevent overfitting. Our computers were millions of times too slow_ We need a lot of computational power to train massive neural networks. used good 90s. deep\n\nCS 486/686 Lecture 21 A brief history of deep learning 13 We initialized the weights in stupid way Weights should be chosen in clever way rather than randomly. We used the wrong type of non-linearity. We should have used different activation function_"
}