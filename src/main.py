import sys
from pathlib import Path
import json

# Adicionar o diret√≥rio raiz ao sys.path para imports relativos funcionarem
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.extractors.pdf_extractor import TextExtractor
from config.settings import INPUT_DIR, OUTPUT_DIR
import logging


def main():
    """Fun√ß√£o principal para teste do extractors"""
    logging.basicConfig(level=logging.INFO)

    print(f"Diret√≥rio atual: {Path.cwd()}")
    print(f"INPUT_DIR: {INPUT_DIR}")
    print(f"OUTPUT_DIR: {OUTPUT_DIR}")
    print(f"INPUT_DIR exists: {INPUT_DIR.exists()}")

    # Inicializar extractors
    extractor = TextExtractor()
    print(f"üîß Formato de sa√≠da configurado: {extractor.config.output_format}")

    # Seu arquivo espec√≠fico
    pdf_path = INPUT_DIR / "lec21_history_neural_networks_typednotes.pdf"

    if pdf_path.exists():
        print(f"üìÑ Extraindo texto de: {pdf_path.name}")
        result = extractor.extract_from_file(pdf_path)

        if result.success:
            print(f"‚úÖ Extra√ß√£o bem-sucedida!")
            print(f"üìä P√°ginas: {result.page_count}")
            print(f"üìù Formato: {result.output_format}")

            # Mostrar estat√≠sticas do JSON simplificado
            if isinstance(result.text_content, list):
                total_chars = sum(len(page_data.get("text", "")) for page_data in result.text_content)
                pages_with_content = len([p for p in result.text_content if p.get("text", "").strip()])

                print(f"üìà Estat√≠sticas JSON:")
                print(f"   - Total de p√°ginas: {len(result.text_content)}")
                print(f"   - P√°ginas com conte√∫do: {pages_with_content}")
                print(f"   - Total de caracteres: {total_chars}")
                print(f"   - P√°ginas vazias: {len(result.text_content) - pages_with_content}")

                # Mostrar amostra da primeira p√°gina com conte√∫do
                first_page_with_content = next(
                    (page for page in result.text_content if page.get("text", "").strip()),
                    None
                )

                if first_page_with_content:
                    sample_text = first_page_with_content["text"][:200]
                    page_num = first_page_with_content["page_num"]
                    print(f"üìã Amostra da p√°gina {page_num}:")
                    print(f"   {sample_text}...")

                # Demonstrar m√©todos utilit√°rios
                print(f"\nüîç Demonstra√ß√£o de funcionalidades:")

                # Buscar um termo
                search_results = extractor.search_in_pages(result, "neural")
                print(f"   - P√°ginas com 'neural': {len(search_results)}")

                # Obter texto de p√°gina espec√≠fica
                page_1_text = extractor.get_page_text(result, 1)
                print(f"   - Caracteres na p√°gina 1: {len(page_1_text)}")

                # P√°ginas com conte√∫do
                content_pages = extractor.get_pages_with_content(result)
                print(f"   - P√°ginas com conte√∫do: {content_pages}")

            else:
                print(f"üìù Caracteres extra√≠dos: {len(result.text_content)}")
                print(f"üìã Primeiros 200 caracteres:")
                print(f"   {result.text_content[:200]}...")

            # Salvar resultado
            output_file = OUTPUT_DIR / f"{pdf_path.stem}_extracted.json"
            success = extractor.save_extracted_text(result, output_file)

            if success:
                print(f"üíæ Arquivo salvo em: {output_file}")

                # Mostrar estrutura do arquivo JSON salvo
                print(f"üîç Estrutura do JSON salvo:")
                try:
                    with open(output_file, 'r', encoding='utf-8') as f:
                        saved_data = json.load(f)

                    if isinstance(saved_data, list):
                        print(f"   - Formato: Array de p√°ginas")
                        print(f"   - Total de p√°ginas: {len(saved_data)}")

                        if saved_data:
                            first_page = saved_data[0]
                            print(f"   - Estrutura da p√°gina: {list(first_page.keys())}")
                            print(f"   - Primeira p√°gina n√∫mero: {first_page.get('page_num', '?')}")
                            print(f"   - Caracteres na primeira p√°gina: {len(first_page.get('text', ''))}")

                        # Mostrar p√°ginas com mais conte√∫do
                        pages_by_size = sorted(
                            saved_data,
                            key=lambda x: len(x.get('text', '')),
                            reverse=True
                        )[:3]

                        print(f"   - Top 3 p√°ginas por conte√∫do:")
                        for i, page in enumerate(pages_by_size, 1):
                            chars = len(page.get('text', ''))
                            page_num = page.get('page_num', '?')
                            print(f"     {i}. P√°gina {page_num}: {chars} caracteres")

                except Exception as e:
                    print(f"   ‚ùå Erro ao ler arquivo salvo: {e}")

        else:
            print(f"‚ùå Erro na extra√ß√£o: {result.error_message}")
    else:
        print(f"‚ùå Arquivo n√£o encontrado: {pdf_path}")
        print("üìÅ Arquivos dispon√≠veis no diret√≥rio input:")
        for file in INPUT_DIR.glob("*.pdf"):
            print(f"   - {file.name}")

    # Processar todos os PDFs do diret√≥rio
    print(f"\nüîç Processando todos os PDFs de: {INPUT_DIR}")
    try:
        results = extractor.extract_from_directory(INPUT_DIR)

        print(f"üìä Resumo do processamento:")
        successful = sum(1 for r in results if r.success)
        failed = len(results) - successful

        print(f"   ‚úÖ Sucessos: {successful}")
        print(f"   ‚ùå Falhas: {failed}")

        for result in results:
            status = "‚úÖ" if result.success else "‚ùå"
            name = Path(result.file_path).name
            format_info = f"({result.output_format})" if hasattr(result, 'output_format') else ""
            print(f"   {status} {name} {format_info}")

        # Mostrar estat√≠sticas detalhadas
        if results:
            summary = extractor.get_extraction_summary(results)
            print(f"\nüìà Estat√≠sticas detalhadas:")
            print(f"   - Total de p√°ginas: {summary['total_pages']}")
            print(f"   - Total de caracteres: {summary['total_characters']}")
            print(f"   - M√©dia de p√°ginas por arquivo: {summary['average_pages']:.1f}")
            print(f"   - Formato de sa√≠da: {summary['output_format']}")

    except Exception as e:
        print(f"‚ùå Erro ao processar diret√≥rio: {str(e)}")


def demo_json_structure():
    """Demonstra a estrutura do JSON simplificado gerado"""
    print("\n" + "=" * 50)
    print("üìã ESTRUTURA DO JSON SIMPLIFICADO:")
    print("=" * 50)

    example_structure = [
        {
            "page_num": 1,
            "text": "CS 486/686 Lecture 21 A brief history of deep learning\n\nThis summary is based on A 'Brief' History of Neural Nets and Deep Learning by Andrew Kurenkov..."
        },
        {
            "page_num": 2,
            "text": "The output is 1 if the weighted sum is big enough and the output is 0 otherwise.\n\nActivation function: a non-linear function..."
        },
        {
            "page_num": 3,
            "text": "A simple algorithm to learn a perceptron:\n1. Start with random weights in a perceptron..."
        }
    ]

    print(json.dumps(example_structure, indent=2, ensure_ascii=False))
    print("=" * 50)


def demo_usage_examples():
    """Demonstra como usar o JSON extra√≠do"""
    print("\n" + "=" * 50)
    print("üí° EXEMPLOS DE USO DO JSON EXTRA√çDO:")
    print("=" * 50)

    example_code = '''
# Carregar JSON extra√≠do
import json
with open('output.json', 'r', encoding='utf-8') as f:
    pages = json.load(f)

# 1. Acessar p√°gina espec√≠fica
page_1 = pages[0]  # Primeira p√°gina (√≠ndice 0)
page_1_text = page_1["text"]
page_1_num = page_1["page_num"]

# 2. Buscar em todas as p√°ginas
def search_term(pages, term):
    results = []
    for page in pages:
        if term.lower() in page["text"].lower():
            results.append({
                "page_num": page["page_num"], 
                "snippet": page["text"][:100] + "..."
            })
    return results

neural_pages = search_term(pages, "neural")

# 3. Estat√≠sticas
total_chars = sum(len(page["text"]) for page in pages)
avg_chars_per_page = total_chars / len(pages)

# 4. Filtrar p√°ginas por tamanho
large_pages = [p for p in pages if len(p["text"]) > 1000]

# 5. Converter para texto √∫nico
full_text = "\\n\\n".join(page["text"] for page in pages)

# 6. Processar com LLMs (exemplo conceitual)
for page in pages:
    if len(page["text"]) > 500:  # S√≥ p√°ginas com conte√∫do substancial
        # Enviar page["text"] para LLM para an√°lise
        pass
'''

    print(example_code)
    print("=" * 50)


if __name__ == "__main__":
    main()

    # Mostrar estrutura de exemplo e usos
    demo_json_structure()
    demo_usage_examples()